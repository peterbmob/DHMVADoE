{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlb43L6TNpON//fzDTF2eV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterbmob/DHMVADoE/blob/main/Excercises/class_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of DoE\n",
        "In this jupyter book, we use the data from the video and perform the analysis with our python tools."
      ],
      "metadata": {
        "id": "0FlYZ-aAkDn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We startwith importing the needed modules, followed by loading the data."
      ],
      "metadata": {
        "id": "Hae35kk44-s1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig7Uc0y2kCLP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data=pd.read_excel('data_example.xlsx', index_col=0, header=1)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the excel file we loaded the data from included empty rows, we need to clean it. this can be done using the drop function in pandas."
      ],
      "metadata": {
        "id": "YAZZnBMC5Pdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.drop(['Unnamed: 4', 'Unnamed: 5', 'Unnamed: 11'], axis=1)\n",
        "data"
      ],
      "metadata": {
        "id": "9GA6uU6vl4yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the video, the DoE was alreay performed, so he already had the design matrix. Let's do it, it makes it prettier and easier to follow.First, let's find the span and label of each variable:"
      ],
      "metadata": {
        "id": "wmddDC3M5yGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dictionary for parameters\n",
        "inputs_labels = {'A' : data.columns[0],\n",
        "                 'B' :  data.columns[1],\n",
        "                 'C' :  data.columns[2]}\n",
        "\n",
        "#create list of data for high and low.\n",
        "dat = [ ('A',data.loc[:,'Time'].min(),data.loc[:,'Time'].max()),\n",
        "        ('B',data.loc[:,'Temp'].min(),data.loc[:,'Temp'].max()),\n",
        "        ('C',data.loc[:,'Pressure'].min(),data.loc[:,'Pressure'].max())]\n",
        "\n",
        "# create pandas dataframe in a pandas dataframe\n",
        "inputs_df = pd.DataFrame(dat,columns=['index','low','high'])\n",
        "inputs_df = inputs_df.set_index(['index'])\n",
        "inputs_df['label'] = inputs_df.index.map( lambda z : inputs_labels[z] )\n",
        "\n",
        "#print dataframe\n",
        "inputs_df"
      ],
      "metadata": {
        "id": "MgMSlLhtmq4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and encode the data usng the span and the averages:"
      ],
      "metadata": {
        "id": "B27iZ-G358Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute averages and span\n",
        "inputs_df['average'] = inputs_df.apply( lambda z : ( z['high'] + z['low'])/2 , axis=1)\n",
        "inputs_df['span'] = inputs_df.apply( lambda z : ( z['high'] - z['low'])/2 , axis=1)\n",
        "\n",
        "# encode the data\n",
        "inputs_df['encoded_low'] = inputs_df.apply( lambda z : ( z['low']  - z['average'] )/( z['span'] ), axis=1)\n",
        "inputs_df['encoded_high'] = inputs_df.apply( lambda z : ( z['high'] - z['average'] )/( z['span'] ), axis=1)\n",
        "\n",
        "inputs_df = inputs_df.drop(['average','span'],axis=1)\n",
        "\n",
        "inputs_df"
      ],
      "metadata": {
        "id": "zRNeTzyvkoeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and generate the design matrix:"
      ],
      "metadata": {
        "id": "Lq8XPBzE6mgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "encoded_inputs= list(itertools.product([-1,1],[-1,1],[-1,1]))\n",
        "encoded_inputs"
      ],
      "metadata": {
        "id": "XEotiyFXoLD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=pd.DataFrame(encoded_inputs)\n",
        "results=results[results.columns[::-1]]\n",
        "results.columns=['A','B','C']\n",
        "results"
      ],
      "metadata": {
        "id": "K4rS6LNUrztr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "which gives the following matrix for the experiments to be performed:"
      ],
      "metadata": {
        "id": "U3HCgB-C6ygQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_experiment = results\n",
        "\n",
        "var_labels = []\n",
        "for var in ['A','B','C']:\n",
        "    var_label = inputs_df.loc[var]['label']\n",
        "    var_labels.append(var_label)\n",
        "    real_experiment[var_label] = results.apply(\n",
        "        lambda z : inputs_df.loc[var]['low'] if z[var]<0 else inputs_df.loc[var]['high'] ,\n",
        "        axis=1)\n",
        "\n",
        "print(\"The values of each real variable in the experiment:\")\n",
        "real_experiment[var_labels]"
      ],
      "metadata": {
        "id": "tJjcbnrbsAMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the order is changed from the minitab example. However, this will not the results. The choice is upt to you, I prefer to divide it like this :)."
      ],
      "metadata": {
        "id": "Oc664nmd7BfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Look us also have closer look at response\n",
        "\n",
        "In the data table, the average y and standard deviation is already given... however, if they are not we can easily compute them using pandas and numpy."
      ],
      "metadata": {
        "id": "ke_P9EOasPLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y=data.loc[:,'Y1':'Y5']\n",
        "# We already had the average y and standard deviation in the table, if not, these are easily accessible pandas and numpy.\n",
        "y_mean=y.mean(axis=1)\n",
        "y_std = y.std(axis=1)\n",
        "results['y']=y_mean.to_list()\n",
        "results['s']=y_std.to_list()\n",
        "results"
      ],
      "metadata": {
        "id": "M996QIz9sNzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These values will of course be the same as the one in the original data table"
      ],
      "metadata": {
        "id": "mky9HcQnoHsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[['Average','Sigma']]"
      ],
      "metadata": {
        "id": "y5cgFtCeMK2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Let's analyse the results\n",
        "In this example we will use statmodels and its \"fit to formula function\" to analyze the results. Since we are looking at the column for the average the full model since then we will not have any degrees of freedom to make a statistical analysis of our model. It is still possibble to perform the OLS with the full model; you just won't get any statistics for your model (test this yourself).\n",
        "\n",
        "How to reduce the model?\n",
        "Here, we often use domain expertise... and expercience. In physics and chemistry, a general rule of thumb is that higher order intercations can be neglected. In this first shot, we therefore remove the **ABC** interaction.\n",
        "\n",
        "We start looking at the standard variation:"
      ],
      "metadata": {
        "id": "Ct6Bhk1yoOYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "\n",
        "dat=results.loc[:,'A':'C']\n",
        "dat['s']=results.loc[:,'s']\n",
        "\n",
        "mod = smf.ols(formula='s ~ A + B + C + A:B + A:C + B:C', data=dat)\n",
        "\n",
        "res = mod.fit()\n",
        "res.summary()"
      ],
      "metadata": {
        "id": "BERtMZ5_16VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see, none of the terms are significant (high p-values) indicating that the model is not of any use for us.\n",
        "\n",
        "\n",
        "Lets turn to the response (y):"
      ],
      "metadata": {
        "id": "E0a78z8v8beF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "\n",
        "dat=results.loc[:,'A':'C']\n",
        "dat['y']=results.loc[:,'y']\n",
        "\n",
        "mod = smf.ols(formula='y ~ A + B + C + A:B + A:C + B:C', data=dat)\n",
        "\n",
        "res = mod.fit()\n",
        "res.summary()"
      ],
      "metadata": {
        "id": "J-bRCrQvvmu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: test to add the **ABC** interaction. When we use the full model, we do not have any Degrees of freedom to evaluate residuals, i.e. **Df Residuals:\t0**. But we can still use this data to analyze the standardized effects of our variables and interactions between variables. Don't forget to reduce the model before you continue with the following boxes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yryNk2SZYyna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the model for y we have A and B that show p-values smaller than 0.05, which means that they significantly contribute to the model. The rest are larger and are thus not significant.\n",
        "\n",
        "This can also bee analyzed in a graphical form. To do this, we summarize the data and plot the results in a **Pareto chart**. This is done by computing the standardized effects and the cumulative percentage of standardized effects, i.e. the percentage of how much of the total effect each individual effect contributes with.  "
      ],
      "metadata": {
        "id": "p5RezYUd86nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effects=pd.DataFrame()\n",
        "\n",
        "# the standardised effects are the absolute values of the coefficients obtained from the OLS.\n",
        "effects['Standardized effect']=pd.DataFrame(np.abs(res.params[1:]))\n",
        "\n",
        "# Add cumulative percentage column.\n",
        "effects[\"cum_percentage\"] = round(effects[\"Standardized effect\"].cumsum()/effects[\"Standardized effect\"].sum()*100,2)\n",
        "effects"
      ],
      "metadata": {
        "id": "XciyjPoCxwI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is best illustrated using a results in a Pareto chart."
      ],
      "metadata": {
        "id": "9izwDSMqrCqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "# Set figure and axis\n",
        "fig, ax = plt.subplots(figsize=(22,10))\n",
        "\n",
        "# Plot bars (i.e. frequencies)\n",
        "ax.set_title(\"Pareto Chart\")\n",
        "ax.set_xlabel(\"Parameter\")\n",
        "ax.set_ylabel(\"Frequency\");\n",
        "effects['Standardized effect'].plot.bar(ax=ax)\n",
        "\n",
        "# Second y axis (i.e. cumulative percentage)\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(effects.index, effects[\"cum_percentage\"], color=\"red\", marker=\"D\", ms=7)\n",
        "effects.plot(y=\"cum_percentage\", color=\"red\", marker=\"D\", ms=7, ax=ax2)\n",
        "ax2.yaxis.set_major_formatter(PercentFormatter())\n",
        "ax2.set_ylabel(\"Cumulative Percentage\");\n",
        "ax2.axhline(80, color=\"orange\", linestyle=\"dashed\")\n"
      ],
      "metadata": {
        "id": "YO9vuH4byU1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The orange line is the Pareto Principle of *\"Vital few\"* and *\"Trivial many\"*, also called the 80/20 tule. The “vital few” are contributing to 80% of the problem, therefore, they should take the highest priority when determining areas to improve as they will give you the “biggest bang for your buck”. Since the “trivial many” only represent 20% of the problem, you will not see much improvement if you focus on these areas. This is the idea behind the 80/20 rule which we will dive deeper into below.\n",
        "\n",
        "To better illustrate the results, it might be wise to sort the data. This can be done with the **sort_values** function in pandas.\n",
        "\n"
      ],
      "metadata": {
        "id": "k2hKsfhdX0pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effects_sorted = effects.sort_values(by='Standardized effect', ascending=False)\n",
        "effects_sorted"
      ],
      "metadata": {
        "id": "extQ6tG5hiYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and be visualized in a Pareto chart"
      ],
      "metadata": {
        "id": "fypPSFd81p9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cumulative percentage column.\n",
        "effects_sorted[\"cum_percentage\"] = round(effects_sorted[\"Standardized effect\"].cumsum()/effects_sorted[\"Standardized effect\"].sum()*100,2)\n",
        "\n",
        "# and plot Pareto Chart\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "# Set figure and axis\n",
        "fig, ax = plt.subplots(figsize=(22,10))\n",
        "\n",
        "# Plot bars (i.e. frequencies)\n",
        "ax.set_title(\"Pareto Chart\")\n",
        "ax.set_xlabel(\"Parameter\")\n",
        "ax.set_ylabel(\"Frequency\");\n",
        "effects_sorted['Standardized effect'].plot.bar(ax=ax)\n",
        "\n",
        "# Second y axis (i.e. cumulative percentage)\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(effects_sorted.index, effects_sorted[\"cum_percentage\"], color=\"red\", marker=\"D\", ms=7)\n",
        "effects_sorted.plot(y=\"cum_percentage\", color=\"red\", marker=\"D\", ms=7, ax=ax2)\n",
        "ax2.yaxis.set_major_formatter(PercentFormatter())\n",
        "ax2.set_ylabel(\"Cumulative Percentage\");\n",
        "ax2.axhline(80, color=\"orange\", linestyle=\"dashed\")\n"
      ],
      "metadata": {
        "id": "j9DR8V2MhvKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we clearly se that A and C are the important parameters in our model. Let's use them and create the final model."
      ],
      "metadata": {
        "id": "25cOss2_9wXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod = smf.ols(formula='y ~ A + C', data=dat)\n",
        "\n",
        "res = mod.fit()\n",
        "res.summary()"
      ],
      "metadata": {
        "id": "voRJaClozEkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a model, and we can ask it to give test the model... Let's start with the real data..."
      ],
      "metadata": {
        "id": "47ZlEmt3-8ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results['y']"
      ],
      "metadata": {
        "id": "1ootXjw1XZ6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "ypred = res.predict(results.loc[:,'A':'C'])\n",
        "#print(ypred)\n",
        "\n",
        "d = pd.DataFrame({'y_pred':ypred,'y_real':results['y']})\n",
        "d.plot.scatter(x='y_real', y='y_pred')\n",
        "\n",
        "x=np.linspace(-5,160,101)\n",
        "\n",
        "plt.plot(x,x,'k-') # identity line"
      ],
      "metadata": {
        "id": "zNRKNl79-2Bt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}