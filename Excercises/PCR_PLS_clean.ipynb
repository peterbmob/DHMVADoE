{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1L95YoZy+peEmxCfmr44J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterbmob/DHMVADoE/blob/main/Excercises/PCR_PLS_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCR and PLS  \n",
        "\n",
        "In this example, we will compare the PCR and PLS methods. Our goal is to illustrate how PLS can outperform PCR when the target is strongly correlated with some directions in the data that have a low variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "OT8UoxEmoolu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we load the data and perform a PCA."
      ],
      "metadata": {
        "id": "41_1IvTBpFyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "food = pd.read_csv(\"https://userpage.fu-berlin.de/soga/data/raw-data/food-texture.csv\")\n",
        "# exclude first column\n",
        "food = food.iloc[:, 1:]\n",
        "\n",
        "\n",
        "scaler = StandardScaler().fit(food)\n",
        "food_scaled = scaler.transform(food)\n",
        "food_scaled=pd.DataFrame(food_scaled,columns=food.columns)\n",
        "food_scaled.head()"
      ],
      "metadata": {
        "id": "jXEX4hp5pXSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets choose a target... we want to predict the Hardness."
      ],
      "metadata": {
        "id": "U05fkFx_natC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jwFcpCGBolmg"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "food_pca = PCA().fit(food_scaled[['Oil',\t'Density', 'Crispy', 'Fracture']])\n",
        "\n",
        "\n",
        "food_pca_eigen = pd.DataFrame(\n",
        "    food_pca.components_.T,\n",
        "    columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\"], #, \"PC5\"],\n",
        "    index=food_scaled[['Oil',\t'Density', 'Crispy', 'Fracture']].columns,\n",
        ")\n",
        "\n",
        "# Compute the loadings\n",
        "food_pca_data=pd.DataFrame()\n",
        "food_pca_data[\"PC\"]=food_pca_eigen.columns\n",
        "food_pca_data[\"Explained Variance\"] = food_pca.explained_variance_\n",
        "food_pca_data[\"Explained Variance Ratio\"] = food_pca.explained_variance_ratio_\n",
        "food_pca_data.set_index('PC')\n",
        "\n",
        "# Compute the scores\n",
        "food_pca_scores = pd.DataFrame(\n",
        "    food_pca.transform(food_scaled[['Oil',\t'Density', 'Crispy', 'Fracture']]),\n",
        "    columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\" ], #, \"PC5\"],\n",
        "    index=food_scaled.index,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the scores\n",
        "sns.scatterplot(data=food_pca_scores, x=\"PC1\", y=\"PC2\")\n",
        "plt.axhline(0, color=\"blue\")\n",
        "plt.axvline(0, color=\"green\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XxCpLSjkqcED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ob813tiKrNG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the scores\n",
        "sns.scatterplot(x=food_pca_scores[\"PC1\"], y=food_scaled['Hardness'])\n",
        "plt.axhline(0, color=\"blue\")\n",
        "plt.axvline(0, color=\"green\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lqn88CQUqwNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the scores\n",
        "sns.scatterplot(x=food_pca_scores[\"PC2\"], y=food_scaled['Hardness'])\n",
        "plt.axhline(0, color=\"blue\")\n",
        "plt.axvline(0, color=\"green\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RwNTQqzJrvnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "rng = np.random.RandomState(0)\n",
        "\n",
        "X_train=food_scaled[['Oil',\t'Density', 'Crispy', 'Fracture']]\n",
        "y_train=food_scaled['Hardness']\n",
        "\n",
        "\n",
        "pcr = make_pipeline(PCA(n_components=1), LinearRegression())\n",
        "pcr.fit(X_train, y_train)\n",
        "pca = pcr.named_steps[\"pca\"]  # retrieve the PCA step of the pipeline\n",
        "\n",
        "pls = PLSRegression(n_components=1)\n",
        "pls.fit(X_train, y_train)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "axes[0].scatter(pca.transform(X_train), y_train, alpha=0.3, label=\"ground truth\")\n",
        "axes[0].scatter(\n",
        "    pca.transform(X_train), pcr.predict(X_train), alpha=0.3, label=\"predictions\"\n",
        ")\n",
        "axes[0].set(\n",
        "    xlabel=\"Projected data onto first PCA component\", ylabel=\"y\", title=\"PCR / PCA\"\n",
        ")\n",
        "axes[0].legend()\n",
        "axes[1].scatter(pls.transform(X_train), y_train, alpha=0.3, label=\"ground truth\")\n",
        "axes[1].scatter(\n",
        "    pls.transform(X_train), pls.predict(X_train), alpha=0.3, label=\"predictions\"\n",
        ")\n",
        "axes[1].set(xlabel=\"Projected data onto first PLS component\", ylabel=\"y\", title=\"PLS\")\n",
        "axes[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VWJMJyt5ou0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PCR r-squared {pcr.score(X_train, y_train):.3f}\")\n",
        "print(f\"PLS r-squared {pls.score(X_train, y_train):.3f}\")"
      ],
      "metadata": {
        "id": "ZHlB0dtKpIOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unsupervised PCA transformation of PCR has dropped the second component, i.e. the direction with the lowest variance, despite it being the most predictive direction. This is because PCA is a completely unsupervised transformation, and results in the projected data having a low predictive power on the target.\n",
        "\n",
        "On the other hand, the PLS regressor manages to capture the effect of the direction with the lowest variance, thanks to its use of target information during the transformation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TX9AdcUFqM-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_2 = make_pipeline(PCA(n_components=2), LinearRegression())\n",
        "pca_2.fit(X_train, y_train)\n",
        "pls_2 = PLSRegression(n_components=2)\n",
        "pls_2.fit(X_train, y_train)\n",
        "\n",
        "print(f\"PCR r-squared with 2 components {pca_2.score(X_train, y_train):.3f}\")\n",
        "print(f\"PLS r-squared with 2 components {pls_2.score(X_train, y_train):.3f}\")"
      ],
      "metadata": {
        "id": "LSwX1TBBpO4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_3 = make_pipeline(PCA(n_components=3), LinearRegression())\n",
        "pca_3.fit(X_train, y_train)\n",
        "pls_3 = PLSRegression(n_components=3)\n",
        "pls_3.fit(X_train, y_train)\n",
        "\n",
        "print(f\"PCR r-squared with 3 components {pca_3.score(X_train, y_train):.3f}\")\n",
        "print(f\"PLS r-squared with 3 components {pls_3.score(X_train, y_train):.3f}\")"
      ],
      "metadata": {
        "id": "dPcTruhUpXCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_4 = make_pipeline(PCA(n_components=3), LinearRegression())\n",
        "pca_4.fit(X_train, y_train)\n",
        "pls_4 = PLSRegression(n_components=3)\n",
        "pls_4.fit(X_train, y_train)\n",
        "\n",
        "print(f\"PCR r-squared with 3 components {pca_4.score(X_train, y_train):.3f}\")\n",
        "print(f\"PLS r-squared with 3 components {pls_4.score(X_train, y_train):.3f}\")"
      ],
      "metadata": {
        "id": "e9YHZ-2RpjEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the maximum r-squared we can get with this data set, and that is when we use all our datapoints in the training.\n",
        "\n",
        "\n",
        "In the example above, we miss a couple of things... We did not to the test-train split as one always should do. Let's try that...\n"
      ],
      "metadata": {
        "id": "j0RFivkWpspI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "rng = np.random.RandomState(0)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(food_scaled[['Oil',\t'Density', 'Crispy', 'Fracture']], food_scaled['Hardness'],test_size=0.2, random_state=rng)\n",
        "\n",
        "\n",
        "pcr = make_pipeline(StandardScaler(), PCA(n_components=1), LinearRegression())\n",
        "pcr.fit(X_train, y_train)\n",
        "pca = pcr.named_steps[\"pca\"]  # retrieve the PCA step of the pipeline\n",
        "\n",
        "pls = PLSRegression(n_components=1)\n",
        "pls.fit(X_train, y_train)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "axes[0].scatter(pca.transform(X_test), y_test, alpha=0.3, label=\"ground truth\")\n",
        "axes[0].scatter(\n",
        "    pca.transform(X_test), pcr.predict(X_test), alpha=0.3, label=\"predictions\"\n",
        ")\n",
        "axes[0].set(\n",
        "    xlabel=\"Projected data onto first PCA component\", ylabel=\"y\", title=\"PCR / PCA\"\n",
        ")\n",
        "axes[0].legend()\n",
        "axes[1].scatter(pls.transform(X_test), y_test, alpha=0.3, label=\"ground truth\")\n",
        "axes[1].scatter(\n",
        "    pls.transform(X_test), pls.predict(X_test), alpha=0.3, label=\"predictions\"\n",
        ")\n",
        "axes[1].set(xlabel=\"Projected data onto first PLS component\", ylabel=\"y\", title=\"PLS\")\n",
        "axes[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8tsWQ_w0rngl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PCR r-squared {pcr.score(X_test, y_test):.3f}\")\n",
        "print(f\"PLS r-squared {pls.score(X_test, y_test):.3f}\")"
      ],
      "metadata": {
        "id": "bTai-PF4s3PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that we indeed get a better performance for the PLS model, but we get quite bad models. The PLS regressor manages to capture the effect of the direction with the lowest variance, thanks to its use of target information during the transformation: it can recognize that this direction is actually the most predictive. We note that the first PLS component is negatively correlated with the target, how can this be?  \n",
        "\n",
        "R$^2$  itself is defined as follows:\n",
        "\n",
        "R$^2$ = 1 - $\\frac{SS_{res}}{SS_{tot}}$ ...\n",
        "\n",
        "and should lie between 0 and 1. if we get negative values, it must mean that $SS_{res}$ > $SS_{tot}$. Here, $SS_{Tot}$ represent the total variation in data, measured by the sum of squares of the difference between expected and actual values, computeted as\n",
        "\n",
        "$SS_{Tot} = SS_{exp} + SS_{res} + ... $    \n",
        "\n",
        "$SS_{exp}$ is the explained sum of squares prepresent and represent the variation in data explained by the fitted model. $SS_{res}$ is the residual sum of squares which represent the variation in data that is not explained by the fitted model. How can this be negative?\n",
        "\n",
        "Well, because we evaluate models separately on train and test data. Following the above definitions, $SS_{tot}$ can be calculated using just the data itself, while $SS_{res}$ depends both on model predictions and the data. While we can use any arbitrary model to generate the predictions for scoring, we need to realize that the aforementioned equality is defined for models trained on the same data. Therefore, it doesn’t necessarily hold true when we use test data to evaluate models built on train data! There is no guarantee that the differences between a foreign model’s predictions and the data is smaller than the variation within the data itself.\n",
        "\n",
        "In short, R$^2$ is only the square of correlation if we happen to be (1) using linear regression models, and (2) are evaluating them on the same data they are fitted (as established previously).\n",
        "\n",
        "\n",
        "We can also look at the mean absolute error and the mean squared error..."
      ],
      "metadata": {
        "id": "hwmze0X-3Rfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred_PCR= pcr.predict(X_test)\n",
        "y_pred_PSL= pls.predict(X_test)\n",
        "\n",
        "\n",
        "print(f\"PCR MAE with 1 component {mean_absolute_error(y_test, y_pred_PCR):.3f}\")\n",
        "print(f\"PLS MAE with 1 component {mean_absolute_error(y_test, y_pred_PSL):.3f}\")\n",
        "print(f\"PCR MSE with 1 component {mean_squared_error(y_test, y_pred_PCR):.3f}\")\n",
        "print(f\"PLS MSE with 1 component {mean_squared_error(y_test, y_pred_PSL):.3f}\")"
      ],
      "metadata": {
        "id": "_lS0_KPp8APc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_2 = make_pipeline(PCA(n_components=2), LinearRegression())\n",
        "pca_2.fit(X_train, y_train)\n",
        "pls_2 = PLSRegression(n_components=2)\n",
        "pls_2.fit(X_train, y_train)\n",
        "\n",
        "print(f\"PCR r-squared with 2 components {pca_2.score(X_test, y_test):.3f}\")\n",
        "print(f\"PLS r-squared with 2 components {pls_2.score(X_test, y_test):.3f}\")"
      ],
      "metadata": {
        "id": "K05j79v-tiB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_3 = make_pipeline(PCA(n_components=3), LinearRegression())\n",
        "pca_3.fit(X_train, y_train)\n",
        "pls_3 = PLSRegression(n_components=3)\n",
        "pls_3.fit(X_train, y_train)\n",
        "\n",
        "print(f\"PCR r-squared with 3 components {pca_3.score(X_test, y_test):.3f}\")\n",
        "print(f\"PLS r-squared with 3 components {pls_3.score(X_test, y_test):.3f}\")"
      ],
      "metadata": {
        "id": "gzvXupgTtqec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_4 = make_pipeline(PCA(n_components=4), LinearRegression())\n",
        "pca_4.fit(X_train, y_train)\n",
        "pls_4 = PLSRegression(n_components=4)\n",
        "pls_4.fit(X_train, y_train)\n",
        "\n",
        "print(f\"PCR r-squared with 3 components {pca_4.score(X_test, y_test):.3f}\")\n",
        "print(f\"PLS r-squared with 3 components {pls_4.score(X_test, y_test):.3f}\")"
      ],
      "metadata": {
        "id": "MIxJzVLYwKQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred_PCR= pca_2.predict(X_test)\n",
        "y_pred_PSL= pls_2.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"PCR MAE with 1 component {mean_absolute_error(y_test, y_pred_PCR):.3f}\")\n",
        "print(f\"PLS MAE with 1 component {mean_absolute_error(y_test, y_pred_PSL):.3f}\")\n",
        "print(f\"PCR MSE with 1 component {mean_squared_error(y_test, y_pred_PCR):.3f}\")\n",
        "print(f\"PLS MSE with 1 component {mean_squared_error(y_test, y_pred_PSL):.3f}\")"
      ],
      "metadata": {
        "id": "bbtkXe1i98qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So when is the PLS better that PCR?\n",
        "\n",
        "PLS can outperform PCR when the target is strongly correlated with some directions in the data that have a low variance. Lets look at a toy example, again doing the PCA followed by PCR and PLS.\n",
        "\n",
        "Lets look at the following data and do a PCA on it:"
      ],
      "metadata": {
        "id": "chNxVnjL_M_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "rng = np.random.RandomState(0)\n",
        "n_samples = 500\n",
        "cov = [[3, 3], [3, 4]]\n",
        "X = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)\n",
        "pca = PCA(n_components=2).fit(X)\n",
        "\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.3, label=\"samples\")\n",
        "for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n",
        "    comp = comp * var  # scale component by its variance explanation power\n",
        "    plt.plot(\n",
        "        [0, comp[0]],\n",
        "        [0, comp[1]],\n",
        "        label=f\"Component {i}\",\n",
        "        linewidth=5,\n",
        "        color=f\"C{i + 2}\",\n",
        "    )\n",
        "plt.gca().set(\n",
        "    aspect=\"equal\",\n",
        "    title=\"2-dimensional dataset with principal components\",\n",
        "    xlabel=\"first feature\",\n",
        "    ylabel=\"second feature\",\n",
        ")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "laY8d_os_mPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We now define the target y such that it is strongly correlated with a direction that has a small variance. To this end, we will project X onto the second component, and add some noise to it."
      ],
      "metadata": {
        "id": "mXvrfQSa_6zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = X.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "\n",
        "axes[0].scatter(X.dot(pca.components_[0]), y, alpha=0.3)\n",
        "axes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\n",
        "axes[1].scatter(X.dot(pca.components_[1]), y, alpha=0.3)\n",
        "axes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OPQnprm-_6QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the PCR and PLS on hits data:"
      ],
      "metadata": {
        "id": "2XI8nZHHAFcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n",
        "\n",
        "pcr = make_pipeline(StandardScaler(), PCA(n_components=1), LinearRegression())\n",
        "pcr.fit(X_train, y_train)\n",
        "pca = pcr.named_steps[\"pca\"]  # retrieve the PCA step of the pipeline\n",
        "\n",
        "pls = PLSRegression(n_components=1)\n",
        "pls.fit(X_train, y_train)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "axes[0].scatter(pca.transform(X_test), y_test, alpha=0.3, label=\"ground truth\")\n",
        "axes[0].scatter(\n",
        "    pca.transform(X_test), pcr.predict(X_test), alpha=0.3, label=\"predictions\"\n",
        ")\n",
        "axes[0].set(\n",
        "    xlabel=\"Projected data onto first PCA component\", ylabel=\"y\", title=\"PCR / PCA\"\n",
        ")\n",
        "axes[0].legend()\n",
        "axes[1].scatter(pls.transform(X_test), y_test, alpha=0.3, label=\"ground truth\")\n",
        "axes[1].scatter(\n",
        "    pls.transform(X_test), pls.predict(X_test), alpha=0.3, label=\"predictions\"\n",
        ")\n",
        "axes[1].set(xlabel=\"Projected data onto first PLS component\", ylabel=\"y\", title=\"PLS\")\n",
        "axes[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MCr9nSEtAEXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen above, the unsupervised PCA transformation of PCR has dropped the second component, i.e. the direction with the lowest variance, despite it being the most predictive direction. This is because PCA is a completely unsupervised transformation, and results in the projected data having a low predictive power on the target.\n",
        "\n",
        "On the other hand, the PLS regressor manages to capture the effect of the direction with the lowest variance, thanks to its use of target information during the transformation: it can recognize that this direction is actually the most predictive. We note that the first PLS component is negatively correlated with the target, which comes from the fact that the signs of eigenvectors are arbitrary.\n",
        "\n",
        "We also print the R-squared scores of both estimators, which further confirms that PLS is a better alternative than PCR in this case. A negative R-squared indicates that PCR performs worse than a regressor that would simply predict the mean of the target."
      ],
      "metadata": {
        "id": "C_ZWDSk_AVJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PCR r-squared {pcr.score(X_test, y_test):.3f}\")\n",
        "print(f\"PLS r-squared {pls.score(X_test, y_test):.3f}\")"
      ],
      "metadata": {
        "id": "L5LOIJcXATu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and the corresponfding MAE and MSE"
      ],
      "metadata": {
        "id": "2h6HdtGaAtcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_PCR= pcr.predict(X_test)\n",
        "y_pred_PSL= pls.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"PCR MAE with 1 component {mean_absolute_error(y_test, y_pred_PCR):.3f}\")\n",
        "print(f\"PLS MAE with 1 component {mean_absolute_error(y_test, y_pred_PSL):.3f}\")\n",
        "print(f\"PCR MSE with 1 component {mean_squared_error(y_test, y_pred_PCR):.3f}\")\n",
        "print(f\"PLS MSE with 1 component {mean_squared_error(y_test, y_pred_PSL):.3f}\")"
      ],
      "metadata": {
        "id": "CIzUyOAkAmq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As previously, we can increase more components which should make the model better:"
      ],
      "metadata": {
        "id": "yd1CoJngAyEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_2 = make_pipeline(PCA(n_components=2), LinearRegression())\n",
        "pca_2.fit(X_train, y_train)\n",
        "print(f\"PCR r-squared with 2 components {pca_2.score(X_test, y_test):.3f}\")"
      ],
      "metadata": {
        "id": "BsvQrsooA3WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note that PCR with 2 components performs as well as PLS: this is because in this case, PCR was able to leverage the second component which has the most preditive power on the target.\n",
        "\n",
        "### Conclusion\n",
        "Indeed, the dimensionality reduction of PCA projects the data into a lower dimensional space where the variance of the projected data is greedily maximized along each axis. Despite them having the most predictive power on the target, the directions with a lower variance will be dropped, and the final regressor will not be able to leverage them.\n",
        "\n",
        "PLS is both a transformer and a regressor, and it is quite similar to PCR: it also applies a dimensionality reduction to the samples before applying a linear regressor to the transformed data. The main difference with PCR is that the PLS transformation is supervised. Therefore, as we will seen in this example, it does not suffer from the issue we just mentioned."
      ],
      "metadata": {
        "id": "YuqYSUbCBS_J"
      }
    }
  ]
}