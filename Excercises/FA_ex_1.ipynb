{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRJPMZZvz8uOc3PJdnc5nI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterbmob/DHMVADoE/blob/main/Excercises/FA_ex_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factor Analysis\n",
        "\n",
        "Adapted from The E-Learning project SOGA-Py was developed at the Department of Earth Sciences by Annette Rudolph, Joachim Krois and Kai Hartmann. You can reach us via mail by soga[at]zedat.fu-berlin.de."
      ],
      "metadata": {
        "id": "TDtfjr93-H_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Factor analysis is an application of latent variable models. The purpose of the analysis is to determine how many latent variables are needed to explain the correlations between the manifest variable, to interpret them and, sometimes, to predict the values of the latent variables which have given rise to the manifest variables.\n",
        "\n",
        "The basic idea behind factor analysis is that we investigate a multivariate feature space covered by a set of observable variables and that we describe the variability and correlation among the observed variables in terms of a potentially lower number of unobserved variables denoted as factors.\n",
        "\n",
        "There are two main branches of factor analysis:\n",
        "\n",
        "- [Confirmatory factor analysis (CFA)](https://en.wikipedia.org/wiki/Confirmatory_factor_analysis)\n",
        "\n",
        "- [Exploratory factor analysis (EFA)](https://en.wikipedia.org/wiki/Exploratory_factor_analysis)\n",
        "\n",
        "In EFA we allow all *m* factors to be related to all *p* observed variables. Thus, one can say that we are exploring which factors relate to which observed variables. Whereas in CFA we know, or assume, based on an a priori hypothesized model that *k* observed variables are related to a particular factor (latent variable). Thus, one can say we try to confirm if a set of *k* observed variables is in fact related to a particular factor.\n",
        "\n",
        "Read more on the math behind Factor analysis on the studium page.\n",
        "\n",
        "In the following tutorial, we focus on **exploratory factor analysis**."
      ],
      "metadata": {
        "id": "IKPpevVJoy1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factor Analysis in Python\n"
      ],
      "metadata": {
        "id": "XASBXd8gqBNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example we compute a factor analysis, employing the `scikit-learn` library.\n",
        "\n",
        "We assume that our data was generated by a linear transformation of a lower dimensional data set, with an overlay of white noise. The factor analysis allows us to retrieve these underlying factors and thus to lower the dimensionality of our data.\n",
        "\n",
        "Let's import the needed tools and get going. We will give further explanation along the way.\n"
      ],
      "metadata": {
        "id": "-BXa80RP95m5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h74QLQrP-G2h"
      },
      "outputs": [],
      "source": [
        "from pandas import read_csv, Series, DataFrame\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The data set\n",
        "Let us get our hands dirty and apply a factor analysis on a the food-texture data set. We already discussed the data set in the section on principal component analysis, so you are probably familiar with the data set.\n",
        "\n",
        "For convenience, we use gain the \"pastry\" dataset we earölier encountered in the PCA tutorial."
      ],
      "metadata": {
        "id": "C208aI4N-CaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "food = read_csv(\"https://userpage.fu-berlin.de/soga/300/30100_data_sets/food-texture.csv\", index_col=0)\n",
        "food.head()"
      ],
      "metadata": {
        "id": "LNPky-fs-mX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data set consists of 50 rows (observations) and 5 columns (features/variables). The features are:\n",
        "\n",
        "- Oil: percentage oil in the pastry\n",
        "- Density: the product’s density (the higher the number, the more dense the product)\n",
        "- Crispy: a crispiness measurement, on a scale from 7 to 15, with 15 being more crispy.\n",
        "- Fracture: the angle, in degrees, through which the pasty can be slowly bent before it fractures.\n",
        "- Hardness: a sharp point is used to measure the amount of force required before breakage occurs."
      ],
      "metadata": {
        "id": "DNup17Sw-f-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The FactorAnalysis class of scikit-learn\n",
        "The class FactorAnalysis of the scikit-learn package, enables many methods around Factor analysis. When instantiating the class, we can pass it the desired number of factors.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "FactorAnalysis(n_components = <factors>)\n",
        "```\n",
        "Replace $<factors>$ with the amount of desired factors.\n",
        "\n",
        "With this class we can perform a maximum-likelihood factor analysis on a covariance matrix or data matrix, specifying the desired number of factors. By an additional argument `rotation` the transformation of the factors may be specified to be either `varimax`or `quartimax`, two types of orthogonal rotation or None (default) for no rotation.\n",
        "\n",
        "Getting a reasonable value for the amount of factors desired is a tricky aspect of factor analysis. If we already have some understanding of the system that created our data, we may make an make an educated guess about the number of latent variables.\n",
        "\n",
        "If we do not know much about out data other than that the number of variables is not too large, we may simply try several values to initialize the model. In most cases though and to do our due dilligence, we should use a more sophisticated approach and perform a principal component analysis (PCA) (as we already did) and get a good initial estimate of the number of factors.\n",
        "\n",
        "Since we have already explained the PCA, we will not repeat it here and just make a guess and set the number of factor to be factors = 2. Furthermore, we try the analysis with the rotation set to varimax and with the default value rotation = None.\n",
        "\n",
        "Let's compute this and plot it using matplotlib."
      ],
      "metadata": {
        "id": "XfHgO16iMDpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = StandardScaler().fit_transform(food)  # Standardize the data\n",
        "factors = 2 # BAsed on previous PCA\n",
        "#  a list of 2 tuples containing titles for and instances of or class\n",
        "fas = [\n",
        "    (\"FA no rotation\", FactorAnalysis(n_components = factors)),\n",
        "    (\"FA varimax\", FactorAnalysis(n_components = factors, rotation=\"varimax\")),\n",
        "]\n",
        "\n",
        "#  Let's prepare some plots on one canvas (subplots)\n",
        "fig, axes = plt.subplots(ncols=len(fas), figsize=(10, 8))\n",
        "\n",
        "'''\n",
        "And loop over the variants of our analysis `fas`, zipped with the\n",
        "plot axes `axes`\n",
        "'''\n",
        "for ax, (title, fa) in zip(axes, fas):\n",
        "    #  Fit the model to the standardized food data\n",
        "    fa = fa.fit(X)\n",
        "    #  and transpose the component (loading) matrix\n",
        "    factor_matrix = fa.components_.T\n",
        "    #  Plot the data as a heat map\n",
        "    im = ax.imshow(factor_matrix, cmap=\"RdBu_r\", vmax=1, vmin=-1)\n",
        "    #  and add the corresponding value to the center of each cell\n",
        "    for (i,j), z in np.ndenumerate(factor_matrix):\n",
        "        ax.text(j, i, str(z.round(2)), ha=\"center\", va=\"center\")\n",
        "    #  Tell matplotlib about the metadata of the plot\n",
        "    ax.set_yticks(np.arange(len(food.columns)))\n",
        "    if ax.get_subplotspec().is_first_col():\n",
        "        ax.set_yticklabels(food.columns)\n",
        "    else:\n",
        "        ax.set_yticklabels([])\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks([0, 1])\n",
        "    ax.set_xticklabels([\"Factor 1\", \"Factor 2\"])\n",
        "    #  and squeeze the axes tight, to save space\n",
        "    plt.tight_layout()\n",
        "\n",
        "#  and add a colorbar\n",
        "cb = fig.colorbar(im, ax=axes, location='right', label=\"loadings\")\n",
        "#  show us the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aM5ujO0s-qMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation of the results\n",
        "Before we interpret the results of the factor analysis, let us recall the basic idea behind it. Factor analysis creates **linear combinations of factors** to abstract the variable’s underlying **communality**. This reduces the amount of factors in the data set, while preserving most of the variance.\n",
        "\n",
        "This allows us to aggregate a large number of observable variables in a model to **represent an underlying concept**, making it easier to comprehend the data.\n",
        "\n",
        "The variability in a data set **X**, is given by **Σ**, and its estimate $$\\hat{\\sum}$$ is composed of the variability explained by a **linear combination of the factors**, which we call **communality**, and of the remaining variability that can not be explained by a linear combination of the factors, namley the **uniqueness**."
      ],
      "metadata": {
        "id": "ioN4cFfUMsJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's have a look at our results:\n",
        "\n",
        "In the plot above we can see the **loadings**, which range from −1 to 1. This part is represented by $$\\hat{\\Lambda}$$ in the equation above. The loadings are the contribution of each original variable to the factor. Variables with loading values further away from **0** have a larger part of their variability factor.\n",
        "\n",
        "Now we will check the **uniqueness** for of each variable. Note that we only use the varimax rotation method, going forward.\n",
        "\n",
        "The next plot is generated straight from a `PandasSeries`, using the `.plot()` method. This provides easy access to quick plots."
      ],
      "metadata": {
        "id": "Z0N3xwGHNU5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fa = FactorAnalysis(n_components = 2, rotation=\"varimax\")\n",
        "fa.fit(X)\n",
        "uniqueness = Series(fa.noise_variance_, index=food.columns)\n",
        "uniqueness.plot(\n",
        "    kind=\"bar\",\n",
        "    ylabel=\"Uniqueness\"\n",
        ")"
      ],
      "metadata": {
        "id": "NTRRH3bH-wKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **uniqueness**, which ranges from 0 to 1. It is, sometimes also referred to as (white) noise, corresponds to the proportion of variability that can not be explained by a linear combination of the factors. This part is represented by the $$\\hat{\\Psi}$$ in the equation above. A high uniqueness for a variable indicates that the factors do not account well for its variance.\n",
        "\n",
        "Opposing the uniquess, stands the **communality**:"
      ],
      "metadata": {
        "id": "TZxrnSOANqtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Communality\n",
        "communality = Series(np.square(fa.components_.T).sum(axis=1), index=food.columns)\n",
        "communality.plot(\n",
        "    kind=\"bar\",\n",
        "    ylabel=\"communality\"\n",
        ")"
      ],
      "metadata": {
        "id": "kR_fNDYk-5PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By squaring the loading we can compute the **fraction of the variable’s total variance explained by the factors**. This proportion of the variability is denoted as **communality**.\n",
        "\n",
        "A way way to calculate the the uniqueness, when you already computed the communlity is to subtract it from 1.**An appropriate factor model results in low values for uniqueness and high values for communality**. So if we see bad results for our model, we could try a different number of underlying factors (latent variables)."
      ],
      "metadata": {
        "id": "uKivQlp-N2zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#and back to uniqueness\n",
        "(1 - communality).plot(kind=\"bar\", ylabel=\"uniqueness\")"
      ],
      "metadata": {
        "id": "XcB-8OnT-59G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the factor analysis model:\n",
        "\n",
        "$$ \\hat{\\sum} = \\hat{\\Lambda} \\hat{\\Lambda}^T + \\hat{\\Psi} $$\n",
        "\n",
        "Using our factor model fa we may calculate\n",
        "$\\hat{\\sum}$ and compare it to the observed correlation matrix, **S**, by simple matrix algebra.\n",
        "\n",
        "We use numpy to perform fast and efficient math operations. Note: We can also use pandas, since it \"wraps\" around numpy, basically just forwarding our commands to the library."
      ],
      "metadata": {
        "id": "IOgGns1MOERX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the word 'lambda' is reserved for a python-operator, so we use the underscore at the end\n",
        "lambda_ = fa.components_\n",
        "psi = np.diag(uniqueness)\n",
        "s = np.corrcoef(np.transpose(X))\n",
        "sigma = np.matmul(lambda_.T, lambda_) + psi\n",
        "residuals = (s - sigma)"
      ],
      "metadata": {
        "id": "OPc-ZC5B_AwH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We subtracted the fitted correlation matrix $\\hat{\\sum}(sigma)$ from the observed correlation matrix **S**. The resulting matrix is called the residual matrix. Numbers close to `0` indicate that our factor model is a good representation of the underlying system. Now lets plot the results."
      ],
      "metadata": {
        "id": "q6HdwF8BNq-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = plt.axes()\n",
        "im = ax.imshow(residuals, cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
        "ax.tick_params(axis=\"x\", bottom=False, labelbottom=False, top=True, labeltop=True)\n",
        "ax.set_xticks(range(5))\n",
        "ax.set_xticklabels(food.columns)\n",
        "ax.set_yticks(range(5))\n",
        "ax.set_yticklabels(food.columns)\n",
        "for (i,j), z in np.ndenumerate(residuals):\n",
        "    ax.text(j, i, str(z.round(3)), ha=\"center\", va=\"center\")\n",
        "\n",
        "fig.colorbar(im, ax=ax, location='right')\n",
        "ax.set_title(\"FA residual matrix\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "8g8219Kn_EiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation of the factors\n",
        "The purpose of a **rotation** is to produce **more extreme loadings**. The idea behind this is to give meaning to the factors. This can help with their interpretation. From a mathematical viewpoint, there is no difference between a rotated and unrotated matrix. **The fitted model is the same, the uniquenesses are the same**, and the proportion of variance explained is the same.\n",
        "\n",
        "Here we fit three factor models, one with `no rotation`, one with `varimax` rotation, and one with `quartimax` rotation. We then make a `scatter plot` of the first and second loadings."
      ],
      "metadata": {
        "id": "p_8YF6r_OaD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\n",
        "    (\"FA No rotation\", FactorAnalysis(2,)),\n",
        "    (\"FA Varimax\", FactorAnalysis(2, rotation=\"varimax\")),\n",
        "    (\"FA Quartimax\", FactorAnalysis(2, rotation=\"quartimax\")),\n",
        "]\n",
        "fig, axes = plt.subplots(ncols=3, figsize=(10, 8), sharex=True, sharey=True)\n",
        "\n",
        "for ax, (method, fa) in zip(axes, methods):\n",
        "    fa = fa.fit(X)\n",
        "\n",
        "    components = fa.components_\n",
        "\n",
        "    vmax = np.abs(components).max()\n",
        "    ax.scatter(components[0,:], components[1, :])\n",
        "    ax.axhline(0, -1, 1, color='k')\n",
        "    ax.axvline(0, -1, 1, color='k')\n",
        "    for i,j, z in zip(components[0, :], components[1, :], food.columns):\n",
        "        ax.text(i+.02, j+.02, str(z), ha=\"center\")\n",
        "    ax.set_title(str(method))\n",
        "    if ax.get_subplotspec().is_first_col():\n",
        "        ax.set_ylabel(\"Factor 1\")\n",
        "    ax.set_xlabel(\"Factor 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SXG9Uh15_IG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What does this mean?\n",
        "How can I interpret these factors? If two variables have loadings further away from `0` for the same factor, we know they are related. We want to understand the data in order to give meaningful names to the latent variables.\n",
        "\n",
        "Taking a look at the plot in the middle `FA Varimax` above, it appears that `Factor 1` describes a variable that makes pastry soft, less crisp. This description fits `flaky pastry` rather well.\n",
        "\n",
        "Whereas the loadings in `Factor 2` show high `density` and little `oil`. This would fit the classification of `hot water crust` pastry"
      ],
      "metadata": {
        "id": "frm3lq99O4Vr"
      }
    }
  ]
}