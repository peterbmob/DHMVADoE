{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM872y/FOd9U5Gyjk0ZwkPG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterbmob/DHMVADoE/blob/main/AdvancedRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multivariable regression Tutorial\n",
        "In this last excercise, we will take data handling and model creation one step further and look in to machine learning techniques. The content is adapted from the paper [Machine Learning for Materials Scientists: An Introductory Guide toward Best Practices](https://pubs.acs.org/doi/full/10.1021/acs.chemmater.0c01907)."
      ],
      "metadata": {
        "id": "wy7j8Vq1fy3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data loading, cleanup and processing\n",
        "adapted from\n",
        "The first step to a ML project is to obtain the dataset you will be working with. There are many repositories for materials science-specific data (whether online or offline)---consult the accompanying paper for a list of the more commonly used ones.\n",
        "\n",
        "Once you have identified the repository and dataset you will use for your project, you will have to download it to your local machine, or establish a way to reliably access the dataset. Consult the documentation of the repository for how to do this.\n",
        "\n",
        "For this tutorial, we have collected heat capacity (\n",
        ") data from the [NIST-JANAF Thermochemical Tables](https://doi.org/10.18434/T42S31)."
      ],
      "metadata": {
        "id": "O4hmFBqgfnD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install missing pacjkages\n",
        "!pip install -U ydata-profiling[notebook]==4.0.0 # matplotlib==3.5.1"
      ],
      "metadata": {
        "id": "gkIBdbcxgn8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NzdKuwGBfawu"
      },
      "outputs": [],
      "source": [
        "# load necessarry libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "from ydata_profiling import ProfileReport"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data\n",
        "Using Pandas, we read in the dataset into a DataFrame.\n",
        "\n",
        "We also print the shape of the DataFrame, which indicates the number of rows and columns in this dataset."
      ],
      "metadata": {
        "id": "WHgYMFFLhERO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = r'https://raw.githubusercontent.com/peterbmob/DHMVADoE/main/DATA/cp_data_demo.csv'\n",
        "df = pd.read_csv(url)\n",
        "print(f'Original DataFrame shape: {df.shape}')"
      ],
      "metadata": {
        "id": "IMtGyzyLhAar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examine the data\n",
        "We examine some rows and look at the data's basic statistics.\n",
        "\n",
        "We see that the dataset contains information about the formula, measurement condition (in this case, temperature in K), and the target property, heat capacity (in J/(mol * K))."
      ],
      "metadata": {
        "id": "2X8ULaJGihLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "1p-LK5YLik7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First thing you should notice: we have many observations of the same compound (B2O3) but measured at different measurement conditions, resulting in a different property value.\n",
        "\n",
        "We can get some simple summary statistics of the DataFrame by calling the .describe() method on the database."
      ],
      "metadata": {
        "id": "urLALlIsjoQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "s_XlxY0cjnbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the pandas-profiling library, we can generate a more in-depth report of our starting dataset. Note that generating this profile report might take upwards of 20 seconds."
      ],
      "metadata": {
        "id": "24EH9OV2jwQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(df.copy(), title='Pandas Profiling Report of Cp dataset', html={'style':{'full_width':True}})\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "CGVy7yMZjvt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice a few things from the profile report:\n",
        "\n",
        "- We have some missing cells in the dataset (\"Overview\" tab)\n",
        "- We have some unrealistic Temperature and Heat Capacity values in the dataset (\"Variables\" tab)\n",
        "- We have some missing Temperature, Formula and Heat Capacity values in the dataset (\"Variables\" tab)\n",
        "\n",
        "Also notice that on the \"Overview\" tab, there is the following warning: FORMULA has a high cardinality: 245 distinct values.\n",
        "Cardinality is the number of distinct values in a column of a table, relative to the number of rows in the table.\n",
        "\n",
        "In our dataset, we have a total of 4583 data observations, but only 245 distinct formulae. We will have to keep this in mind later, when we process and split the dataset."
      ],
      "metadata": {
        "id": "xficwoLEkW1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rename the column names for brevity"
      ],
      "metadata": {
        "id": "NS6rX2pqlF7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.columns"
      ],
      "metadata": {
        "id": "sg0m4YZxjtuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rename_dict = {'FORMULA': 'formula',\n",
        "               'CONDITION: Temperature (K)': 'T',\n",
        "               'PROPERTY: Heat Capacity (J/mol K)': 'Cp'}\n",
        "df = df.rename(columns=rename_dict)\n",
        "df.columns"
      ],
      "metadata": {
        "id": "tcKkqXN2lKS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for and remove NaN values"
      ],
      "metadata": {
        "id": "PQHrFgzdlQeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaNs in the respective dataset columns, and get the indices\n",
        "df2 = df.copy()\n",
        "bool_nans_formula = df2['formula'].isnull()\n",
        "bool_nans_T = df2['T'].isnull()\n",
        "bool_nans_Cp = df2['Cp'].isnull()\n",
        "\n",
        "# Drop the rows of the DataFrame which contain NaNs\n",
        "df2 = df2.drop(df2.loc[bool_nans_formula].index, axis=0)\n",
        "df2 = df2.drop(df2.loc[bool_nans_T].index, axis=0)\n",
        "df2 = df2.drop(df2.loc[bool_nans_Cp].index, axis=0)\n",
        "\n",
        "print(f'DataFrame shape before dropping NaNs: {df.shape}')\n",
        "print(f'DataFrame shape after dropping NaNs: {df2.shape}')"
      ],
      "metadata": {
        "id": "8fsUl1TllSfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas also includes the convenient built-in method .dropna() to check for and remove NaNs in-place:\n",
        "\n"
      ],
      "metadata": {
        "id": "BLEWSfHRlYGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df.copy()\n",
        "df3 = df3.dropna(axis=0, how='any')\n",
        "\n",
        "print(f'DataFrame shape before dropping NaNs: {df.shape}')\n",
        "print(f'DataFrame shape after dropping NaNs: {df3.shape}')\n",
        "\n",
        "df = df3.copy()"
      ],
      "metadata": {
        "id": "xiSP1KvblabR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for and remove unrealistic values\n",
        "In some cases, you might also get data values that simply don't make sense. For our dase, this could be negative values in the temperature or heat capacity values."
      ],
      "metadata": {
        "id": "XxLXEXLzlfe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bool_invalid_T = df['T'] < 0\n",
        "bool_invalid_Cp = df['Cp'] < 0\n",
        "\n",
        "df = df.drop(df.loc[bool_invalid_T].index, axis=0)\n",
        "df = df.drop(df.loc[bool_invalid_Cp].index, axis=0)\n",
        "\n",
        "print(f'Cleaned DataFrame shape: {df.shape}')"
      ],
      "metadata": {
        "id": "nbyM_2Mnla3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save cleaned data to csv\n",
        "Finally, after cleaning and processing the data, you can save it to disk in a cleaned state for you to use later.\n",
        "\n",
        "Pandas allows us to save our data as a comma separated value .csv file."
      ],
      "metadata": {
        "id": "d61rVUAVlpdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_path = os.path.join('cp_data_cleaned.csv')\n",
        "df.to_csv(out_path, index=False)"
      ],
      "metadata": {
        "id": "i2ktuTYmlor_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "mxm0-CkelxQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Splitting\n",
        "Splitting data into the train/validation/test dataset\n",
        "It is important to split your full dataset into train/validation/test datasets, and reliably use the same datasets for your modeling tasks later.\n",
        "\n",
        "Using different train/validation/test splits can dramatically affect your model performance (as seen here by the variance in\n",
        " scores for 30 models which have been trained on 30 different dataset splits) [1]:"
      ],
      "metadata": {
        "id": "kN0wnrmEl1O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set a random seed to ensure reproducibility across runs\n",
        "RNG_SEED = 42\n",
        "np.random.seed(seed=RNG_SEED)"
      ],
      "metadata": {
        "id": "PXl08ntgmBFB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the pre-processed dataset\n",
        "\n",
        "We will start with the processed dataset that we saved previously."
      ],
      "metadata": {
        "id": "I8qNmZmZmKp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = os.getcwd()\n",
        "data_path = os.path.join(PATH, 'cp_data_cleaned.csv')\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "print(f'Full DataFrame shape: {df.shape}')"
      ],
      "metadata": {
        "id": "MYP0ux0KmEja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separate the DataFrame into your input variables (X) and target variables (y)\n",
        "The X will be used as the input data, and y will be used as the prediction targets for your ML model.\n",
        "\n",
        "If your target variables are discrete (such as metal/non-metal or types of crystal structures), then you will be performing a classification task. In our case, since our target variables are continuous values (heat capacity), we are performing a regression task."
      ],
      "metadata": {
        "id": "xFjyKprdmXSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['formula', 'T']]\n",
        "y = df['Cp']\n",
        "\n",
        "print(f'Shape of X: {X.shape}')\n",
        "print(f'Shape of y: {y.shape}')"
      ],
      "metadata": {
        "id": "EOwyWNYImjpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting data (and a word of caution)\n",
        "Normally, we could simply split the data with a simple sklearn function\n",
        "The scikit-learn train_test_split function randomly splits a dataset into train and test datasets. Typically, you can use train_test_split to first split your data into \"train\" and \"test\" datasets, and then use the function again to split your \"train\" data into \"train\" and \"validation\" dataset splits.\n",
        "\n",
        "As a rule of thumb, you can roughly aim for the following dataset proportions when splitting your data:\n",
        "\n",
        "| | train split | validation split | test split |\n",
        "| --- | --- | --- | --- |\n",
        "| proportion of original dataset | 50% to 70% | 20% to 30% | 10% to20% |\n",
        "\n",
        "\n",
        "If you have copious amounts of data, it may suffice to train your models on just 50% of the data; that way, you have a larger amount of data samples to validate and to test with. If you however have a smaller dataset and thus very few training samples for your models, you may wish to increase your proportion of training data during dataset splitting."
      ],
      "metadata": {
        "id": "k84TSx6kmp-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RNG_SEED)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "9d-z_ZQkrJxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**But wait, what's wrong here?**\n",
        "We have to make sure that our dataset splits contain mutually exclusive formulae (e.g., all the data samples associated with \"Al2O3\" is either in the train, validation, or test dataset, but not in multiple)!"
      ],
      "metadata": {
        "id": "cqUiPdRVrx62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = len(X_train)\n",
        "print(f'There are in total {num_rows} rows in the X_train DataFrame.')\n",
        "\n",
        "num_unique_formulae = len(X_train['formula'].unique())\n",
        "print(f'But there are only {num_unique_formulae} unique formulae!\\n')\n",
        "\n",
        "print('Unique formulae and their number of occurances in the X_train DataFrame:')\n",
        "print(X_train['formula'].value_counts(), '\\n')\n",
        "print('Unique formulae and their number of occurances in the X_test DataFrame:')\n",
        "print(X_test['formula'].value_counts())"
      ],
      "metadata": {
        "id": "f0AbFiBXr5HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are in total 3651 rows in the X_train DataFrame. But there are only 244 unique formulae! In fact, you will see that the same formulae are often present in the X_train and X_test DataFrames!\n",
        "\n",
        "That's not good, because now we have instances of the same chemical compound appearing in both the training and test data. Which means the model can cheat and in essence just memorize the training data, and during testing, look up the nearby values present in the training data!\n",
        "\n",
        "So how do we mitigate this?"
      ],
      "metadata": {
        "id": "JzA6NNMTr9oJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Be aware of leaking data between datasets**\n",
        "We have to first group the data by chemical formula, then split the data according to the chemical formulae. That way, all data points associated with each formula are either in the training dataset or in the test dataset, but not in both at the same time."
      ],
      "metadata": {
        "id": "u7_kZ9EhsGBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting data, cautiously (manually)\n",
        "First we get a list of all of the unique formulae in the dataset."
      ],
      "metadata": {
        "id": "QFi-77m4sO5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_formulae = X['formula'].unique()\n",
        "print(f'{len(unique_formulae)} unique formulae:\\n{unique_formulae}')"
      ],
      "metadata": {
        "id": "7Wh3AwvXr6s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a random seed to ensure reproducibility across runs\n",
        "np.random.seed(seed=RNG_SEED)\n",
        "\n",
        "# Store a list of all unique formulae\n",
        "all_formulae = unique_formulae.copy()\n",
        "\n",
        "# Define the proportional size of the dataset split\n",
        "val_size = 0.20\n",
        "test_size = 0.10\n",
        "train_size = 1 - val_size - test_size\n",
        "\n",
        "# Calculate the number of samples in each dataset split\n",
        "num_val_samples = int(round(val_size * len(unique_formulae)))\n",
        "num_test_samples = int(round(test_size * len(unique_formulae)))\n",
        "num_train_samples = int(round((1 - val_size - test_size) * len(unique_formulae)))\n",
        "\n",
        "# Randomly choose the formulate for the validation dataset, and remove those from the unique formulae list\n",
        "val_formulae = np.random.choice(all_formulae, size=num_val_samples, replace=False)\n",
        "all_formulae = [f for f in all_formulae if f not in val_formulae]\n",
        "\n",
        "# Randomly choose the formulate for the test dataset, and remove those from the unique formulae list\n",
        "test_formulae = np.random.choice(all_formulae, size=num_test_samples, replace=False)\n",
        "all_formulae = [f for f in all_formulae if f not in test_formulae]\n",
        "\n",
        "# The remaining formulae will be used for the training dataset\n",
        "train_formulae = all_formulae.copy()\n",
        "\n",
        "print('Number of training formulae:', len(train_formulae))\n",
        "print('Number of validation formulae:', len(val_formulae))\n",
        "print('Number of testing formulae:', len(test_formulae))"
      ],
      "metadata": {
        "id": "ilDt72ZgsSvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the original dataset into the train/validation/test datasets using the formulae lists above\n",
        "df_train = df[df['formula'].isin(train_formulae)]\n",
        "df_val = df[df['formula'].isin(val_formulae)]\n",
        "df_test = df[df['formula'].isin(test_formulae)]\n",
        "\n",
        "print(f'train dataset shape: {df_train.shape}')\n",
        "print(f'validation dataset shape: {df_val.shape}')\n",
        "print(f'test dataset shape: {df_test.shape}\\n')\n",
        "\n",
        "print(df_train.head(), '\\n')\n",
        "print(df_val.head(), '\\n')\n",
        "print(df_test.head(), '\\n')"
      ],
      "metadata": {
        "id": "HC08R765sVV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be sure that we really only have mutually exclusive formulae within each of the datasets (e.g., all the data samples associated with \"Al2O3\" is either in the train, validation, or test dataset, but not in multiple), we can do the following to check:"
      ],
      "metadata": {
        "id": "RwB791NGscqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_formulae = set(df_train['formula'].unique())\n",
        "val_formulae = set(df_val['formula'].unique())\n",
        "test_formulae = set(df_test['formula'].unique())\n",
        "\n",
        "common_formulae1 = train_formulae.intersection(test_formulae)\n",
        "common_formulae2 = train_formulae.intersection(val_formulae)\n",
        "common_formulae3 = test_formulae.intersection(val_formulae)\n",
        "\n",
        "print(f'# of common formulae in intersection 1: {len(common_formulae1)}; common formulae: {common_formulae1}')\n",
        "print(f'# of common formulae in intersection 2: {len(common_formulae2)}; common formulae: {common_formulae2}')\n",
        "print(f'# of common formulae in intersection 3: {len(common_formulae3)}; common formulae: {common_formulae3}')"
      ],
      "metadata": {
        "id": "QAgEfpDvsY9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save split datasets to csv\n",
        "Finally, after splitting the dataset into train/validation/test dataset splits, you can save them to disk for you to use later.\n",
        "\n",
        "By saving these dataset splits into files, you can then later reproducibly use these same exact splits during your subsequent model training and comparison steps. Use the same datasets for all your models---that way, you can ensure a fair comparison.\n",
        "\n",
        "Also, when you publish your results, you can include these dataset splits, so that others can use the exact datasets in their own studies."
      ],
      "metadata": {
        "id": "8F1kUAGmsnm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving these splits into csv files\n",
        "PATH = os.getcwd()\n",
        "\n",
        "train_path = os.path.join(PATH, 'cp_train.csv')\n",
        "val_path = os.path.join(PATH, 'cp_val.csv')\n",
        "test_path = os.path.join(PATH, 'cp_test.csv')\n",
        "\n",
        "df_train.to_csv(train_path, index=False)\n",
        "df_val.to_csv(val_path, index=False)\n",
        "df_test.to_csv(test_path, index=False)"
      ],
      "metadata": {
        "id": "2nP1yKSlsm8F"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, keep the test dataset locked away and forget about it until you have finalized your model! **Never look at the test dataset!!**"
      ],
      "metadata": {
        "id": "1JjAoJxus0kZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Featurization\n",
        "Here, we will show some simple examples of featurizing materials composition data using so-called \"composition-based feature vectors\", or CBFVs. This methods represents a single chemical formula as one vector based on its constituent atoms' chemical properties (refer to the paper for more information and references).\n",
        "\n",
        "Note that the steps shown in this notebook are intended to demonstrate the best practices associated with featurizing materials data, using one way of featurizing materials composition data as an example. Depending on your input data and your particular modeling needs, the data featurization method and procedure you use may be different than the example shown here."
      ],
      "metadata": {
        "id": "ekW-v763s3_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As usual, first load necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Set a random seed to ensure reproducibility across runs\n",
        "RNG_SEED = 42\n",
        "np.random.seed(RNG_SEED)"
      ],
      "metadata": {
        "id": "DJWCUMxusfZ_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ],
      "metadata": {
        "id": "28aixd6etQ8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = os.getcwd()\n",
        "train_path = os.path.join(PATH, 'cp_train.csv')\n",
        "val_path = os.path.join(PATH, 'cp_val.csv')\n",
        "test_path = os.path.join(PATH, 'cp_test.csv')\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_val = pd.read_csv(val_path)\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "print(f'df_train DataFrame shape: {df_train.shape}')\n",
        "print(f'df_val DataFrame shape: {df_val.shape}')\n",
        "print(f'df_test DataFrame shape: {df_test.shape}')"
      ],
      "metadata": {
        "id": "Qos6vguPtPF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sub-sampling your data (optional)\n",
        "If your dataset is too large, you can subsample it to be a smaller size. This is useful for prototyping and for making quick sanity tests of new models / parameters.\n",
        "\n",
        "Just be aware that you do not introduce any bias into your data through the sampling."
      ],
      "metadata": {
        "id": "1v0gblV0tacc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sub-sample the data. Set the random_state to make the sampling reproducible every time.\n",
        "df_train_sampled = df_train.sample(n=2000, random_state=RNG_SEED)\n",
        "df_val_sampled = df_val.sample(n=200, random_state=RNG_SEED)\n",
        "df_test_sampled = df_test.sample(n=200, random_state=RNG_SEED)\n",
        "\n",
        "print(f'df_train_sampled DataFrame shape: {df_train_sampled.shape}')\n",
        "print(f'df_val_sampled DataFrame shape: {df_val_sampled.shape}')\n",
        "print(f'df_test_sampled DataFrame shape: {df_test_sampled.shape}')"
      ],
      "metadata": {
        "id": "UXeByrGltXKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate features using the CBFV package\n",
        "To featurize the chemical compositions from a chemical formula (e.g. \"Al2O3\") into a composition-based feature vector (CBFV), we use the open-source [CBFV package](https://github.com/kaaiian/CBFV).\n",
        "\n",
        "To use it, we need to download the package. This we can do by cloning the git repository to our colab environment.\n"
      ],
      "metadata": {
        "id": "8tSYpTpAtj3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Kaaiian/CBFV.git"
      ],
      "metadata": {
        "id": "PJMPqT-jtfaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we can import the module"
      ],
      "metadata": {
        "id": "pBCzm-cyuBye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the package and the generate_features function\n",
        "from CBFV.cbfv.composition import generate_features"
      ],
      "metadata": {
        "id": "Ulib01aEuAKe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generate_features function from the CBFV package expects an input DataFrame containing at least the columns ['formula', 'target']. You may also have extra feature columns (e.g., temperature or pressure, other measurement conditions, etc.).\n",
        "\n",
        "In our dataset, Cp represents the target variable, and T is the measurement condition. Since the generate_features function expects the target variable column to be named target, we have to rename the Cp column."
      ],
      "metadata": {
        "id": "13hVWuRHuKrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('DataFrame column names before renaming:')\n",
        "print(df_train.columns)\n",
        "print(df_val.columns)\n",
        "print(df_test.columns)\n",
        "\n",
        "rename_dict = {'Cp': 'target'}\n",
        "df_train = df_train.rename(columns=rename_dict)\n",
        "df_val = df_val.rename(columns=rename_dict)\n",
        "df_test = df_test.rename(columns=rename_dict)\n",
        "\n",
        "df_train_sampled = df_train_sampled.rename(columns=rename_dict)\n",
        "df_val_sampled = df_val_sampled.rename(columns=rename_dict)\n",
        "df_test_sampled = df_test_sampled.rename(columns=rename_dict)\n",
        "\n",
        "print('\\nDataFrame column names after renaming:')\n",
        "print(df_train.columns)\n",
        "print(df_val.columns)\n",
        "print(df_test.columns)"
      ],
      "metadata": {
        "id": "EpO7E8OOuIb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the generate_features function to generate the CBFVs from the input data.\n",
        "\n",
        "Note that we have specified several keyword arguments in our call to generate_features:\n",
        "\n",
        "- elem_prop='oliynyk'\n",
        "- drop_duplicates=False\n",
        "- extend_features=True\n",
        "- sum_feat=True\n",
        "\n",
        "A short explanation for the choice of keyword arguments is below:\n",
        "\n",
        "- The elem_prop parameter specifies which CBFV featurization scheme to use (there are several). For this tutorial, we have chosen to use the oliynyk CBFV featurization scheme.\n",
        "- The drop_duplicates parameter specifies whether to drop duplicate formulae during featurization. In our case, we want to preserve duplicate formulae in our data (True), since we have multiple heat capacity measurements (performed at different temperatures) for the same compound.\n",
        "- The extend_features parameter specifies whether to include extended features (features that are not part of ['formula', 'target']) in the featurized data. In our case, this is our measurement temperature, and we want to include this information (True), since this is pertinent information for the heat capacity prediction.\n",
        "- The sum_feat parameter specifies whether to calculate the sum features when generating the CBFVs for the chemical formulae. We do in our case (True).\n",
        "\n",
        "For more information about the generate_features function and the CBFV featurization scheme, refer to the [GitHub repository](https://github.com/kaaiian/CBFV)."
      ],
      "metadata": {
        "id": "msOeMLUHuhkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_unscaled, y_train, formulae_train, skipped_train = generate_features(df_train_sampled, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
        "X_val_unscaled, y_val, formulae_val, skipped_val = generate_features(df_val_sampled, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
        "X_test_unscaled, y_test, formulae_test, skipped_test = generate_features(df_test_sampled, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)"
      ],
      "metadata": {
        "id": "HkyiRm1Vug2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see what a featurized X matrix looks like, .head() will show us some rows:"
      ],
      "metadata": {
        "id": "EH1DZyz-vIrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_unscaled.head()"
      ],
      "metadata": {
        "id": "PGTZh2vpu_35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_unscaled.shape"
      ],
      "metadata": {
        "id": "0wnjH5lVvKld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the sum features in the CBFV, which we have included by using sum_feat=True in the call to generate_features.\n",
        "\n",
        "Also note the temperature column T at the end of this featurized data.\n",
        "\n",
        "What we have done above is featurize the input data. In the featurized data, each row contains a unique CBFV that describes a given chemical composition."
      ],
      "metadata": {
        "id": "IUH-CD1AvTSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Data scaling & normalization\n",
        "For numerical input data, scaling and normalization of the features often improves the model performance. Scaling can partially correct the discrepancy between the orders of magnitudes of the features (e.g., some numerical features being much larger or smaller than others). This typically improves the model learning performance, and in turn, improves the model performance.\n",
        "\n",
        "We will scale then normalize our input data using scikit-learn's built-in `StandardScaler` class and `normalize` function.\n",
        "\n",
        "Note, in addition to `StandardScaler`, other scalers such as `RobustScaler` and `MinMaxScaler` are also available in scikit-learn. Consult the documentation for the details and when to use them."
      ],
      "metadata": {
        "id": "oJYHZdcnvhNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize"
      ],
      "metadata": {
        "id": "If2Cr3IfvgQj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling the data\n",
        "First, we instantiate the scaler object.\n",
        "\n",
        "In a `StandardScaler` object:\n",
        "\n",
        "- During the `fit` process, the statistics of the input data (mean and standard deviation) are computed.\n",
        "- Then, during the `transform process`, the mean and standard deviation values calculated above are used to scale the data to having zero-mean and unit variance.\n",
        "\n",
        "Therefore, for the first time usage of the scaler, we call the `.fit_transform()` method to fit the scaler to the input data, and then to transform the same data. For subsequent uses, since we have already computed the statistics, we only call the `.transform()` method to scale data.\n",
        "\n",
        "**Note:** you should only `.fit()` the scaler using the training dataset statistics, and then use these same statistics from the training dataset to `.transform()` the other datasets (validation and train)."
      ],
      "metadata": {
        "id": "Yle1GIgavouF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train_unscaled)\n",
        "X_val = scaler.transform(X_val_unscaled)\n",
        "X_test = scaler.transform(X_test_unscaled)"
      ],
      "metadata": {
        "id": "IUeqSjXZvNA2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing the scaled data\n",
        "We repeat a similar process for normalizing the data. Here, there is no need to first fit the normalizer, since the normalizer scales the rows of the input data to unit norm independently of other rows.\n",
        "\n",
        "The normalizer is different to a Scaler in that the normalizer acts row-wise, whereas a Scaler acts column-wise on the input data."
      ],
      "metadata": {
        "id": "gQ9HnzaOwzV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = normalize(X_train)\n",
        "X_val = normalize(X_val)\n",
        "X_test = normalize(X_test)"
      ],
      "metadata": {
        "id": "GAocn0-gw6OU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Making the model\n",
        "Here we will use the multiple multivariate regression from sklearn."
      ],
      "metadata": {
        "id": "beoTUx3QxAub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "# Create linear regression object\n",
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Fit regression model to the training set\n",
        "regr.fit(X_train, y_train)\n",
        ""
      ],
      "metadata": {
        "id": "O9jk0Rt1xg5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = regr.predict(X_train)"
      ],
      "metadata": {
        "id": "oXBmeBVLxkl4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing true versus predicted values\n",
        "plt.scatter(y_train, y_train_pred, color='black')\n",
        "plt.title('Comparing true and predicted values for test set')\n",
        "plt.xlabel('True values for y')\n",
        "plt.ylabel('Predicted values for y')\n",
        "\n",
        "# Model evaluation\n",
        "print(\"Root mean squared error = %.4f\" % np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
        "print('R-squared = %.4f' % r2_score(y_train, y_train_pred))"
      ],
      "metadata": {
        "id": "1dUM2YF8xnz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the question is how good we are on the test set..."
      ],
      "metadata": {
        "id": "XaIC4OHCzWZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = regr.predict(X_val)"
      ],
      "metadata": {
        "id": "GXi7aOy1zfIa"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing true versus predicted values\n",
        "plt.scatter(y_val, y_val_pred, color='black')\n",
        "plt.title('Comparing true and predicted values for test set')\n",
        "plt.xlabel('True values for y')\n",
        "plt.ylabel('Predicted values for y')\n",
        "\n",
        "# Model evaluation\n",
        "print(\"Root mean squared error = %.4f\" % np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
        "print('R-squared = %.4f' % r2_score(y_val, y_val_pred))"
      ],
      "metadata": {
        "id": "-7sNSeZJxqTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Och så hade vi ju vårt testset..."
      ],
      "metadata": {
        "id": "qqH5BDmuz7WA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = regr.predict(X_test)"
      ],
      "metadata": {
        "id": "1mgBl20gz4gx"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing true versus predicted values\n",
        "plt.scatter(y_val, y_test_pred, color='black')\n",
        "plt.title('Comparing true and predicted values for test set')\n",
        "plt.xlabel('True values for y')\n",
        "plt.ylabel('Predicted values for y')\n",
        "\n",
        "# Model evaluation\n",
        "print(\"Root mean squared error = %.4f\" % np.sqrt(mean_squared_error(y_val, y_test_pred)))\n",
        "print('R-squared = %.4f' % r2_score(y_val, y_test_pred))"
      ],
      "metadata": {
        "id": "z_-wXxvV0X33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok för träningsetet, men inte så bra för validering... Kan vi göra det bättre? låt oss testa ridge regression..."
      ],
      "metadata": {
        "id": "hCeVyC0Y0fVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ridge = linear_model.Ridge(alpha=0.8)\n",
        "ridge.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "47C9UP090djv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us again test the performance:"
      ],
      "metadata": {
        "id": "XP7y7s7x1IiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,3, figsize=(15,8))\n",
        "\n",
        "y_train_pred = ridge.predict(X_train)\n",
        "# Comparing true versus predicted values\n",
        "axs[0].scatter(y_train, y_train_pred, color='black')\n",
        "#axs[0].title('Comparing true and predicted values for test set')\n",
        "#axs[0].xlabel('True values for y')\n",
        "#axs[0].ylabel('Predicted values for y')\n",
        "\n",
        "# Model evaluation\n",
        "print('train test')\n",
        "print(\"Root mean squared error = %.4f\" % np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
        "print('R-squared = %.4f' % r2_score(y_train, y_train_pred))\n",
        "\n",
        "\n",
        "y_val_pred = ridge.predict(X_val)\n",
        "# Comparing true versus predicted values\n",
        "axs[1].scatter(y_val, y_val_pred, color='black')\n",
        "#axs[1].title('Comparing true and predicted values for test set')\n",
        "#axs[1].xlabel('True values for y')\n",
        "#axs[1].ylabel('Predicted values for y')\n",
        "\n",
        "# Model evaluation\n",
        "print('val test')\n",
        "print(\"Root mean squared error = %.4f\" % np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
        "print('R-squared = %.4f' % r2_score(y_val, y_val_pred))\n",
        "\n",
        "\n",
        "y_test_pred = ridge.predict(X_test)\n",
        "# Comparing true versus predicted values\n",
        "axs[2].scatter(y_test, y_test_pred, color='black')\n",
        "#axs[2].title('Comparing true and predicted values for test set')\n",
        "#axs[2].xlabel('True values for y')\n",
        "#axs[2].ylabel('Predicted values for y')\n",
        "\n",
        "# Model evaluation\n",
        "print('test test')\n",
        "print(\"Root mean squared error = %.4f\" % np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
        "print('R-squared = %.4f' % r2_score(y_test, y_test_pred))\n",
        "\n",
        "for i in range(3):\n",
        "    axs[i].axis('equal')\n",
        "    axs[i].set_box_aspect(1)"
      ],
      "metadata": {
        "id": "GBr_kYLE0uot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much better!!!\n",
        "\n",
        "\n",
        "However, sk-learn has many options... such as the kernel regression discussed in Lecture 2, can we get better?  \n"
      ],
      "metadata": {
        "id": "DEeJYyt738Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.kernel_ridge import KernelRidge\n",
        "# Kernel ridge regression with RBF\n",
        "gamma=1\n",
        "kernel_ridge_reg = KernelRidge(alpha=0.1, kernel='rbf', gamma=gamma)\n",
        "kernel_ridge_reg.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "C24z1jTV4ZYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,3, figsize=(15,8))\n",
        "\n",
        "y_train_pred = kernel_ridge_reg .predict(X_train)\n",
        "# Comparing true versus predicted values\n",
        "axs[0].scatter(y_train, y_train_pred, color='black')\n",
        "#axs[0].title('Comparing true and predicted values for test set')\n",
        "#axs[0].xlabel('True values for y')\n",
        "#axs[0].ylabel('Predicted values for y')\n",
        "\n",
        "# Model evaluation\n",
        "print('train test')\n",
        "print(\"Root mean squared error = %.4f\" % np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
        "print('R-squared = %.4f' % r2_score(y_train, y_train_pred))\n",
        "\n",
        "\n",
        "y_val_pred = kernel_ridge_reg .predict(X_val)\n",
        "# Comparing true versus predicted values\n",
        "axs[1].scatter(y_val, y_val_pred, color='black')\n",
        "#axs[1].title('Comparing true and predicted values for test set')\n",
        "#axs[1].xlabel('True values for y')\n",
        "#axs[1].ylabel('Predicted values for y')\n",
        "\n",
        "# Model evaluation\n",
        "print('val test')\n",
        "print(\"Root mean squared error = %.4f\" % np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
        "print('R-squared = %.4f' % r2_score(y_val, y_val_pred))\n",
        "\n",
        "\n",
        "y_test_pred = kernel_ridge_reg .predict(X_test)\n",
        "# Comparing true versus predicted values\n",
        "axs[2].scatter(y_test, y_test_pred, color='black')\n",
        "#axs[2].title('Comparing true and predicted values for test set')\n",
        "#axs[2].xlabel('True values for y')\n",
        "#axs[2].ylabel('Predicted values for y')\n",
        "\n",
        "# Model evaluation\n",
        "print('test test')\n",
        "print(\"Root mean squared error = %.4f\" % np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
        "print('R-squared = %.4f' % r2_score(y_test, y_test_pred))\n",
        "\n",
        "for i in range(3):\n",
        "    axs[i].axis('equal')\n",
        "    axs[i].set_box_aspect(1)\n"
      ],
      "metadata": {
        "id": "zIBoQ7DP4bYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even better!\n",
        "\n",
        "What parameters govern the quality? Test to change gamma and alpha in the kernel model and alpha in the ridge model.\n",
        "\n"
      ],
      "metadata": {
        "id": "lmo9oywr8EUy"
      }
    }
  ]
}